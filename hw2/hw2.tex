\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{parskip}


\usepackage[a4paper, margin=1in]{geometry}

% \hfuzz=10pt
% \sbox{\mybox}{\hbadness=10000 \parbox{2cm}{\lipsum[1]}}

\title{S\&DS 351: Stochastic Processes - Homework 2}
\author{Bryan SebaRaj \\[0.8em] Professor Ilias Zadik}
\date{January 31, 2025}

\begin{document}

\maketitle

\subsection*{Problem 1.1 (15 points)}
Suppose that we are given a Markov Chain $X_0, X_1, \ldots$. Show that for any $1 \leq k \leq n-1$, and states $i, j, j_1, j_2, \ldots, j_{n-k}$:
\[
P(X_n = i | X_{n-k} = j, X_{n-k-1} = j_1, X_{n-k-2} = j_2, \ldots, X_0 = j_{n-k}) = P(X_n = i | X_{n-k} = j).
\]
Prove this result and show your steps.

\textcolor{blue}{Suppose }

\subsection*{Problem 1.2 (10 points)} 
Consider the following
coin-tossing game. A single fair coin is flipped sequentially. Alice
wins if \texttt{TTT} comes up first and Bob wins if \texttt{HTT}
comes up. What is the probability that Alice wins?

% \textcolor{blue}{Let $A$ be the event that Alice wins and $B$ be the event that Bob wins. Solving for $P(A)$,}

\textcolor{blue}{Let $S$ be a set of possible states. Define $S_0$ as the starting state with no flips or no partial pattern of TTT or HTT. Define $S_1$ as the state where there has been exactly one tail in a row, i.e. the most recent flip was T, without any prior heads. Define $S_2$ as the state where the last two flips were consecutive tails, without any prior heads. Define $S_A$ as the state where the last three flips were consecutive tails, i.e. the absorbing state where Alice has won. Define $S_B$ as the state where the last three flips were consecutive heads, i.e. the absorbing state where Bob has won. Finally, define $S_H$ to be the state where at least one heads has been seen. \\ This yields the following Markov Chain:}

\begin{center}
\includegraphics[scale=0.11]{/Users/bryansebaraj/Downloads/IMG_2158.jpg}
\end{center}

\textcolor{blue}{This yields the transition matrix,}

    $$P = \begin{bmatrix}
        P_{S_0, S_0} & P_{S_0, S_1} & P_{S_0, S_2} & P_{S_0, S_A} & P_{S_0, S_B} & P_{S_0, S_H}\\
        P_{S_1, S_0} & P_{S_1, S_1} & P_{S_1, S_2} & P_{S_1, S_A} & P_{S_1, S_B} & P_{S_1, S_H}\\
        P_{S_2, S_0} & P_{S_2, S_1} & P_{S_2, S_2} & P_{S_2, S_A} & P_{S_2, S_B} & P_{S_2, S_H}\\
        P_{S_A, S_0} & P_{S_A, S_1} & P_{S_A, S_2} & P_{S_A, S_A} & P_{S_A, S_B} & P_{S_A, S_H}\\
        P_{S_B, S_0} & P_{S_B, S_1} & P_{S_B, S_2} & P_{S_B, S_A} & P_{S_B, S_B} & P_{S_B, S_H}\\
        P_{S_H, S_0} & P_{S_H, S_1} & P_{S_H, S_2} & P_{S_H, S_A} & P_{S_H, S_B} & P_{S_H, S_H}

        
\end{bmatrix}$$

$$P=\begin{bmatrix}
    0 & 0.5 & 0 & 0 & 0 & 0.5 \\
    0 & 0 & 0.5 & 0 & 0 & 0.5 \\ 
    0 & 0 & 0 & 0.5 & 0 & 0.5 \\ 
    0 & 0 & 0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 0 & P_{S_H, S_B}\neq 0 & P_{S_H, S_H}\neq 0
\end{bmatrix}$$

\textcolor{blue}{Note that after $S_H$ has been reached, Alice can no longer win, as HTT will always preceed TTT, i.e. $\lim_{n \rightarrow \infty} \pi_{S_H} = (0, 0, 0, 0, 1, 0)$, or Bob will eventually always win after $S_H$ is reached. Since $S_A$ is the only absorbing state where Alice can win, the probabilty of reaching $S_A$ is given by, \\ 
    $$p_0=\mathbb{P}(S_A | S_0), p_1=\mathbb{P}(S_A | S_1), p_2=\mathbb{P}(S_A | S_2)$$
Using the probability transition matrix and that any transition to $S_H$ prohibits Alice from winning, 
$$p_2 = 1 \cdot P_{S_2, S_A} + 0 \cdot P_{S_2, S_H} = \frac{1}{2}$$
$$p_1 = P_{S_1, S_2} \cdot P_{S_2, S_A} + 0 \cdot P_{S_1, S_H} = \frac{1}{4}$$
$$p_0 = P_{S_0, S_1} \cdot P_{S_1, S_2} \cdot P_{S_2, S_A} + 0 \cdot P_{S_0, S_H} = \frac{1}{8}$$
Thus, the probability that Alice wins is $\frac{1}{8}$.}




\subsection*{Problem 1.3 (Moran model)} 
In population genetics, the
Moran model is a simple Markov chain-based model to describe
evolutionary competition. Consider a population of $N$ individuals,
divided into two types called $A$ and $B$. The population evolves
according to the following mechanism: At each time $n \geq 0$, two
individuals are selected from the current population by independent
random sampling \textit{with replacement}. The first individual gives
birth to a copy of itself, which joins the population together with
its parent. Then, the second individual dies and is removed from the
population. The result of these two steps is the population at time
$n + 1$. The random samplings at different times are all independent
and uniform. Note that by design, the size of the total population
stays at $N$.

\begin{enumerate}
    \item[(a)] (5 points) Let $X_n$ denote the number of individuals of type $A$ at time $n$. Show that $\{X_n : n \geq 0\}$ is a Markov chain. Identify the state space and find the transition probabilities $P = (P_{ij})$.
    \item[(b)] (5 points) For $N = 3$, write down the transition matrix $P$ explicitly. Find the stationary distribution(s), i.e., the distribution(s) $\pi$ with $\pi = P \pi$. Is it unique?
\end{enumerate}

\subsection*{Problem 1.4}
Let $M$ be a positive integer. Let $Y_0, Y_1, \ldots$ be iid and uniformly distributed on $\{1, \ldots, M\}$. Let $X_n = \min\{Y_0, \ldots, Y_n\}$.

\begin{enumerate}
    \item[(a)] (5 points) Show that $\{X_n : n \geq 0\}$ is a Markov chain.
    \item[(b)] (5 points) Identify the state space and find the probability transition matrix $P$.
    \item[(c)] (5 points) Find the stationary distribution $\pi$. Is it unique? Provide an intuitive explanation for your conclusion.
    \item[(d)] (5 points) Let $Y_0 = M$. Prove that the marginal distribution of $X_n$ converges to the stationary distribution, i.e., if $\pi_n(i) = P(X_n = i), i = 1, \ldots, M$ then for all $i = 1, \ldots, M$:
    \[
    \lim_{n \to \infty} \pi_n(i) = \pi(i).
    \]
\end{enumerate}

\section*{Problems from Chang}

\section*{Exercise 1.1}
Let $X_0, X_1, \ldots$ be a Markov chain, and let $A$ and $B$ be subsets of the state space.

\begin{enumerate}
    \item[(a)] Is it true that $P\{X_2 \in B \mid X_1 = x_1, X_0 \in A\} = P\{X_2 \in B \mid X_1 = x_1\}$? Give a proof or counterexample.

    \textcolor{blue}{By the definition of conditional probability,
$$P\{X_2 \in B \mid X_1 = x_1, X_0 \in A\} = \frac{P\{X_2 \in B, X_1 = x_1, X_0 \in A\}}{P\{X_1 = x_1, X_0 \in A\}}$$
Using the law of total probability,
$$P\{X_2 \in B, X_1 = x_1, X_0 \in A\} = \sum_{y \in A} P\{X_2 \in B, X_1 = x_1, X_0 = y\}$$
By the chain rule,
$$P\{X_2 \in B, X_1 = x_1, X_0 = y\} = P\{X_2 \in B \mid X_1 = x_1, X_0 = y\} \cdot P\{X_1 = x_1, X_0 = y\}$$
By the Markov property,
$$P\{X_2 \in B \mid X_1 = x_1, X_0 = y\} = P\{X_2 \in B \mid X_1 = x_1\}$$
Substituting yields,
$$P\{X_2 \in B, X_1 = x_1, X_0 = y\} = P\{X_2 \in B \mid X_1 = x_1\} \cdot P\{X_1 = x_1, X_0 = y\}$$
$$P\{X_2 \in B, X_1 = x_1, X_0 \in A\} = P\{X_2 \in B \mid X_1 = x_1\} \sum_{y \in A} P\{X_1 = x_1, X_0 = y\}$$
Note that $\sum_{y \in A} P\{X_1 = x_1, X_0 = y\} = P\{X_1 = x_1, X_0 \in A\}$ \\
$$P\{X_2 \in B \mid X_1 = x_1, X_0 \in A\} = P\{X_2 \in B \mid X_1 = x_1\} \frac{P\{X_1 = x_1, X_0 \in A\}}{P\{X_1 = x_1, X_0 \in A\}} = P\{X_2 \in B \mid X_1 = x_1\}$$
Therefore, $$P\{X_2 \in B \mid X_1 = x_1, X_0 \in A\} = P\{X_2 \in B \mid X_1 = x_1\}$$}


    \item[(b)] Is it true that $P\{X_2 \in B \mid X_1 \in A, X_0 = x_0\} = P\{X_2 \in B \mid X_1 \in A\}$? Give a proof or counterexample.


    \textcolor{blue}{The above statement is false. Consider the Markov chain
    with states $S = \{0, 1, 2\}$ and transition probabilities: $$P_{0,1}=1,
    P_{1,2}=1, \text{ and } P_{2,0}=1$$where $P_{i,j}$ is the probability of switching
from state $i$ to state $j$ at time $n$. \\ \\ Suppose $A=\{0,1\}$ and $B=\{2\}$ and the initial state, $x_0=0$. \\ \\
In this trivial scenario,
$$P\{X_2 \in B \mid X_1 \in A, X_0 = 0\} = 1$$
If $X_0 = 0$, then $X_1 = 1$, and from state 1 we must go to state 2. Since state 1 is in $A$, the probability of being in $B$ at time 2 is 1. \\
\\However, consider
$$P\{X_2 \in B \mid X_1 \in A\}$$$A$ includes cases where $X_1 = 0$ (which would lead to state 1 at time 2, not state 2).
Therefore, the probability of being in $B$ at time 2 is 0 given $X_1 = 0$. \\
This counterexample shows that knowing the specific value of $X_0$ can provide additional information about which state in $A$ the chain is in at time 1, which affects the probability of being in $B$ at time 2.
Therefore, the statement is false.}

\end{enumerate}

\section*{Exercise 1.3} 
Let $\{X_n\}$ be a finite-state Markov chain
and let $A$ be a subset of the state space. Suppose we want to
determine the expected time until the chain enters the set $A$,
starting from an arbitrary initial state. That is, letting $\tau_A =
\inf\{n \geq 0 : X_n \in A\}$ denote the first time to hit $A$
[defined to be $0$ if $X_0 \in A$], we want to determine
$\mathbb{E}_i(\tau_A)$. Show that \[ \mathbb{E}_i(\tau_A) = 1 + \sum_k P(i, k)
\mathbb{E}_k(\tau_A) \] for $i \notin A$.


\textcolor{blue}{Let $\tau_A$ be defined as the first time the chain enters $A$, where if $X_0\in A, \tau_A=0$ and $X_0\notin A, \tau_A$ is the first time $n \geq 1$ such that $X_n \in A$. \\ \\ Starting from $i\notin A$, the chain must take one step to some state $k$ with probability $P_{i,k}$. Let us define the total expected time to reach $A$ as $$\mathbb{E}(\tau_a)=\mathbb{E}[\tau_a \mid X_0=i]$$
Since at least one step if required to reach $A$, $\tau_A$ can be decomposed as $$\tau_A = 1 + \tau_A'$$ where $\tau_A'$ is the remaining time to reach $A$ after the first step. \\ \\
Applying the law of total expectation conditioned in $X_1=k$, $$\mathbb{E}(\tau_A)=\mathbb{E}[1+\tau_A' \mid X_0=i] = 1 + \mathbb{E}[\tau_A' \mid X_0 =i]$$
By the Markov property, $$\mathbb{E}[\tau_A' \mid  X_0=i] = \sum_k P_{i,k}\mathbb{E}[\tau_A \mid X_1=k]$$
Note that if $k\in A$, then $\tau_A=0$ as $X_1 \in A$, so $\mathbb{E}(\tau_A)=0$. \\ 
If $k\notin A$, then $\tau_A'$ follows the same distribution as $\tau_a$ starting from $k$. Therefore, $$\mathbb{E}[\tau_A' \mid X_1= k]=\mathbb{E}(\tau_A)$$
Thus, $$\mathbb{E}(\tau_A)=1+\sum_k P_{i,k}\mathbb{E}(\tau_A)$$}

\section*{Exercise 1.4} 
You are tossing a coin repeatedly. Which
pattern would you expect to see faster: \texttt{HH} or \texttt{HT}?
For example, if you get the sequence \texttt{TTHHHTH...}, then you
see \texttt{HH} at the 4th toss and \texttt{HT} at the 6th. Letting
$N_1$ and $N_2$ denote the times required to see \texttt{HH} and
\texttt{HT}, respectively, can you guess intuitively whether $\mathbb{E}(N_1)$
is smaller than, the same as, or larger than $\mathbb{E}(N_2)$? Go ahead, make
a guess [and my day]. Why don’t you also simulate some to see how the answer
looks; I recommed a computer, buf if you like tossing real coins, enjoy yourself by all means. Finally, you can use the reasoning of Exercise [1.3] to solve the
problem and evaluate $\mathbb{E}(N_i)$. A hint is to set up a Markov chain
having the 4 states \texttt{HH}, \texttt{HT}, \texttt{TH}, and
\texttt{TT}.

\textcolor{blue}{Intuitively, I would guess that $\mathbb{E}(N_1)=\mathbb{E}(N_2)$, as both events are dependent, and only dependent on the first heads. Since a coin is equally and independently likely to get heads or tails on the next flip, the probabilites are the same (I know this is a naive assumption). \\ \\ 
First, let us define the Markov chain with states $S=\{HH, HT, TH, TT\}$ and transition probabilities: $$P_{HH,HH}=0.5, P_{HH,HT}=0.5, P_{HH,TH}=0, P_{HH,TT}=0$$ $$P_{HT,HH}=0, P_{HT,HT}=0, P_{HT,TH}=0.5, P_{HT,TT}=0.5$$ $$P_{TH,HH}=0.5, P_{TH,HT}=0.5, P_{TH,TH}=0, P_{TH,TT}=0$$ $$P_{TT,HH}=0, P_{TT,HT}=0, P_{TT,TH}=0.5, P_{TT,TT}=0.5$$
Regarding $\mathbb{E}(N_1)$, }

\section*{Exercise 1.6}
[A moving average process] Moving average models are used frequently in time series analysis, economics and engineering. For these models, one assumes that there is an underlying, unobserved process $\ldots, Y_{-1}, Y_0, Y_1, \ldots$ of \textit{iid} random variables. A \textbf{\textit{moving average process}} takes an average (possibly a weighted average) of these \textit{iid} random variables in a “sliding window.” For example, suppose that at time $n$ we simply take the average of the $Y_n$ and $Y_{n-1}$, defining $X_n = \frac{1}{2}(Y_n + Y_{n-1})$. Our goal is to show that the process $X_0, X_1, \ldots$ defined in this way is not Markov. As a simple example, suppose that the distribution of the \textit{iid} $Y$ random variables is $\mathbb{P}\{Y_i = 1\} = 1/2 = \mathbb{P}\{Y_i = -1\}$.

\begin{enumerate}
    \item[(a)] Show that $X_0, X_1, \ldots$ is not a Markov chain.
    \item[(b)] Show that $X_0, X_1, \ldots$ is not an $r$th order Markov chain for any finite $r$.
\end{enumerate}


\end{document}

