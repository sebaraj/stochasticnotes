\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{parskip}


\usepackage[a4paper, margin=1in]{geometry}

\title{S\&DS 351: Stochastic Processes - Homework 8}
\author{Bryan SebaRaj \\[0.8em] Professor Ilias Zadik}
\date{April 18, 2025}

\begin{document}

\maketitle

\textbf{Chang Problems:}

\textbf{[5.8]} \textit{The strong Markov property} is an extension of
the restarting property of Proposition \textbf{5.5} from fixed times
$c$ to random \textit{stopping times} $\gamma$: For a stopping time
$\gamma$, the process $x$ defined by $X(t) = W(\gamma + t) -
W(\gamma)$ is a Brownian motion, independent of the path of $W$ up to
time $\gamma$. Explain the role of the stopping time requirement by
explaining how the restarting property can fail for a random time
that isn’t a stopping time. For example, let $M = \max\{B_t : 0 \leq
t \leq 1\}$ and let $\beta = \inf\{t : B_t = M\}$; this is the first
time at which $B$ achieves its maximum height over the time interval
$[0,1]$. Clearly $\beta$ is not a stopping time, since we must look
at the whole path $\{B_t : 0 \leq t \leq 1\}$ to determine when the
maximum is attained. Argue that the restarted process $X(t) = W(\beta
+ t) - W(\beta)$ is not a standard Brownian motion.

% (answer in one sentence)

\textcolor{blue}{ Since $B_{\beta+t}\le B_\beta, \ \forall t$ s.t. $0\le
    t\le 1-\beta,$ $X(1-\beta)=B_1-B_\beta\le 0$ w.p. 1; this contradicts 
    the symmetry of a $N(0,1-\beta)$ law and
    proves that $X$ cannot be a standard Brownian motion,
    showing why the strong Markov property requires $\beta$ to be
    a stopping time. }

\textbf{[5.9]} \textbf{[Ornstein-Uhlenbeck process]} Define a process $X$ by
\[
X(t) = e^{-t}W(e^{2t})
\]
for $t \geq 0$, where $W$ is a standard Brownian motion. $X$ is called an \textit{Ornstein-Uhlenbeck process}.

\begin{enumerate}
    \item[(a)] Find the covariance function of $X$.

    \textcolor{blue}{
Let the process $X$ be obtained from a standard Brownian motion $W$ by deterministic space–time change as given.
% $$
% X(t)=e^{-t}W\!\bigl(e^{2t}\bigr),\qquad t\ge 0.
% $$
Because $W$ is Gaussian with mean $0$, $X$ is also Gaussian with mean $0$. As such, its second‑order behaviour is described by its covariance function. Suppose $s,t\ge 0$ and WLOG assume $s\le t$. Then,
$$
\mathbb{E}\bigl[X(s)X(t)\bigr]
=\mathbb{E}\!\Bigl[e^{-s}W(e^{2s})\;e^{-t}W(e^{2t})\Bigr]
=e^{-(s+t)}\,
\mathbb{E}\!\Bigl[W(e^{2s})\,W(e^{2t})\Bigr]
$$
Recall that Brownian motion has covariance $\mathbb{E}[W(u)W(v)]=\min\{u,v\}$.
$$
\mathbb{E}\bigl[X(s)X(t)\bigr]
=e^{-(s+t)}\min\!\bigl\{e^{2s},e^{2t}\bigr\}
=e^{-(s+t)}e^{2s}
=e^{-(t-s)}
$$
By symmetry in $(s,t)$, this can be extended to all $s,t\ge 0$, yielding
$$
\operatorname{Cov}\bigl(X(s),X(t)\bigr)=e^{-|t-s|},\qquad s,t\ge 0
$$
% Therefore, $X$ is a stationary centered Gaussian process with exponentially decaying covariance, the hallmark of the Ornstein–Uhlenbeck family.
    }

    \item[(b)] Evaluate the functions $\mu$ and $\sigma^2$, defined by
    \[
    \mu(x,t) = \lim_{h \downarrow 0} \frac{1}{h} \mathbb{E}[X(t+h) - X(t) \mid X(t) = x]
    \]
    \[
    \sigma^2(x,t) = \lim_{h \downarrow 0} \frac{1}{h} \mathrm{Var}[X(t+h) - X(t) \mid X(t) = x].
    \]

    \textcolor{blue}{
Expand $X(t+h)$ around $t$,
$$
\Delta_h=W\!\bigl(e^{2(t+h)}\bigr)-W(e^{2t}),
\qquad\text{so that}\quad
X(t+h)=e^{-(t+h)}\bigl[W(e^{2t})+\Delta_h\bigr]
$$
% \textbf{Conditional distribution of $\Delta_h$.}  
See that since $W$ has independent increments, $\Delta_h$ is independent of $W(e^{2t})$ and is Gaussian with mean $0$ and variance.  
$$
\mathrm{Var}(\Delta_h)=e^{2(t+h)}-e^{2t}=e^{2t}\bigl(e^{2h}-1\bigr)
$$
Also see that when $\{X(t)=x\}$, the value of $W(e^{2t})$ becomes
$$
X(t)=e^{-t}W(e^{2t})=x
$$
$$
W(e^{2t})=e^{t}x
$$
Therefore, in this case,
$$
\mathbb{E}[\Delta_h\,|\,X(t)=x]=0 \quad \text{and}
\quad
\mathrm{Var}[\Delta_h\,|\,X(t)=x]=e^{2t}\bigl(e^{2h}-1\bigr)
$$
Evaluating the first conditional moment,
\[
\begin{aligned}
\mathbb{E}\bigl[X(t+h)-X(t)\,\big|\,X(t)=x\bigr]
&=\mathbb{E}\Bigl[e^{-(t+h)}W(e^{2t})-e^{-t}W(e^{2t})
                 +e^{-(t+h)}\Delta_h\;\Big|\;X(t)=x\Bigr]\\
&=(e^{-(t+h)}-e^{-t})\,e^{t}x+e^{-(t+h)}\mathbb{E}[\Delta_h\,|\,X(t)=x]\\
&=(e^{-h}-1)x
\end{aligned}
\]
Therefore,
$$
\mu(x,t)
=\lim_{h\downarrow 0}\frac{1}{h}\,\mathbb{E}\bigl[X(t+h)-X(t)\,|\,X(t)=x\bigr]
=\lim_{h\downarrow 0}\frac{e^{-h}-1}{h}\,x
=-x
$$
Evaluating the second conditional moment,
\[
\begin{aligned}
\mathrm{Var}\bigl[X(t+h)-X(t)\,\big|\,X(t)=x\bigr]
&=\mathrm{Var}\Bigl[e^{-(t+h)}\Delta_h\Bigr]\\
&=e^{-2(t+h)}\,\mathrm{Var}[\Delta_h\,|\,X(t)=x]\\
&=e^{-2h}\bigl(e^{2h}-1\bigr)\\
% &=2h+o(h)\quad(h\downarrow 0)
\end{aligned}
\]
Therefore,
$$
\sigma^{2}(x,t)
=\lim_{h\downarrow 0}\frac{1}{h}\,
\mathrm{Var}\bigl[X(t+h)-X(t)\,|\,X(t)=x\bigr]
=2
$$
% \textbf{Interpretation.}  The limits $\mu(x,t)=-x$ and $\sigma^{2}(x,t)=2$ coincide with the drift and twice the diffusion coefficient in the stochastic differential equation  
% $$
% dX_t=-X_t\,dt+\sqrt{2}\,dW_t^{\phantom{.}},
% $$
% whose unique stationary solution is precisely the Ornstein–Uhlenbeck process we constructed by the time–space transform of Brownian motion.
    }

\end{enumerate}



\textbf{[5.10]} Let $W$ be a standard Brownian motion.

\begin{enumerate}
    \item[(i)] Defining $\tau_b = \inf\{t : W(t) = b\}$ for $b > 0$ as above, show that $\tau_b$ has probability density function
    \[
    f_{\tau_b}(t) = \frac{b}{\sqrt{2\pi}} t^{-3/2} e^{-b^2/(2t)}
    \]
    for $t > 0$.

    \textcolor{blue}{
By the reflection principle, see that
$$
P\{\tau_b\le t\}=P\Bigl\{\sup_{0\le s\le t}W(s)\ge b\Bigr\}=2P\{W(t)\ge b\}
$$
Since $W(t)\sim\mathcal N(0,t)$,
$$
P\{W(t)\ge b\}=1-\Phi\!\Bigl(\frac{b}{\sqrt t}\Bigr)
$$
where $\Phi$ is the standard normal distribution function and $\varphi(z)=\frac1{\sqrt{2\pi}}e^{-z^{2}/2}$ its density. Therefore,
$$
F_{\tau_b}(t)=2\Bigl[1-\Phi\!\Bigl(\frac{b}{\sqrt t}\Bigr)\Bigr]
\quad \text{when }t>0
$$
Differentiating to obtain the density,
$$
f_{\tau_b}(t)=2\varphi\!\Bigl(\frac{b}{\sqrt t}\Bigr)\Bigl(-\frac{b}{2}t^{-3/2}\Bigr)(-1)
=\frac{b}{\sqrt{2\pi}}\,t^{-3/2}e^{-b^{2}/(2t)},
              \quad \text{when }t>0
$$
Thus, $\tau_b$ has a pdf as claimed.
    }
    
    \item[(ii)] Show that for $0 < t_0 < t_1$,
    \[
    P\{W(t) = 0 \text{ for some } t \in (t_0, t_1)\} = \frac{2}{\pi} \tan^{-1} \left( \sqrt{\frac{t_1}{t_0} - 1} \right) = \frac{2}{\pi} \cos^{-1} \left( \sqrt{\frac{t_0}{t_1}} \right).
    \]
    
    \textit{[Hint: The last equality is simple trigonometry. For the previous equality, condition on the value of $W(t_0)$, use part (i), and Fubini (or perhaps integration by parts).]}

        \textcolor{blue}{
Suppose $0<t_0<t_1$ and define $\Delta=t_1-t_0$. By the Markov property, conditioning on $W(t_0)=x$ and 
$$
\tau_{|x|}=\inf\{s>0:W_{x}(s)=0\}\quad\text{s.t. }\bigl(W_{x}(0)=x\bigr)
$$
yields
$$
P\{W(t)=0, t\in(t_0,t_1)\}
  =\int_{\mathbb R}P\{\tau_{|x|}\le\Delta\}\,
           \frac{e^{-x^{2}/(2t_0)}}{\sqrt{2\pi t_0}}\;dx
$$
Since $\tau_{|x|}$ has the density from part (i) with $b=|x|$,
$$
P\{\tau_{|x|}\le\Delta\}=\int_{0}^{\Delta}\frac{|x|}{\sqrt{2\pi}}s^{-3/2}
e^{-x^{2}/2s}ds
$$
Restricting to $x>0$ based on symmetry,
$$
P = 2\int_{0}^{\infty}\frac{e^{-x^{2}/(2t_0)}}{\sqrt{2\pi t_0}}
         \int_{0}^{\Delta}\frac{x}{\sqrt{2\pi}}s^{-3/2}
               e^{-x^{2}/2s}\,ds\,dx
$$
% Fubini’s theorem allows us to swap the integrals:
$$
P=\frac{2}{2\pi\sqrt{t_0}}
    \int_{0}^{\Delta}s^{-3/2}\!
      \int_{0}^{\infty}x\exp\!\Bigl(-x^{2}\Bigl(\frac1{2t_0}+\frac1{2s}\Bigr)\Bigr)\,dx\,ds
$$
For $\alpha>0$, see that $\int_{0}^{\infty}x e^{-\alpha x^{2}}dx=\frac1{2\alpha}$.
Since
$$
\alpha=\frac1{2t_0}+\frac1{2s}=\frac{s+t_0}{2st_0}
$$
$$
\frac1{2\alpha}=\frac{st_0}{s+t_0}
$$
Therefore,  
$$
P=\frac1{2\pi\sqrt{t_0}}
      \int_{0}^{\Delta}\frac{2t_0\,s^{-1/2}}{s+t_0}\,ds
  =\frac{\sqrt{t_0}}{\pi}\int_{0}^{\Delta}\!\frac{s^{-1/2}}{s+t_0}\,ds 
$$
Let $s=t_0u^{2}$ $(u\ge0)$. As such, $ds=2t_0u\,du$ and the upper limit becomes  
$$
u_{\max}=\sqrt{\frac{\Delta}{t_0}}=\sqrt{\frac{t_1}{t_0}-1}
$$
Substituting this into the integral,
$$
P=\frac{\sqrt{t_0}}{\pi}\int_{0}^{u_{\max}}
       \frac{1}{\sqrt{t_0}u}\,\frac{2t_0u}{t_0(1+u^{2})}\,du
  =\frac{2}{\pi}\int_{0}^{u_{\max}}\frac{du}{1+u^{2}}
  =\frac{2}{\pi}\tan^{-1}(u_{\max})
$$
Given that 
$
u_{\max}=\sqrt{\frac{t_1}{t_0}-1}$
$$
P=\frac{2}{\pi}\tan^{-1}\!\Bigl(\sqrt{\frac{t_1}{t_0}-1}\Bigr)
  =\frac{2}{\pi}\cos^{-1}\!\Bigl(\sqrt{\frac{t_0}{t_1}}\Bigr)
$$
% the last equality being the elementary identity
% $$
% \tan^{-1}\!\bigl(\sqrt{z-1}\bigr)=\cos^{-1}\!\bigl(z^{-1/2}\bigr)\qquad(z>1).
% $$
        }

\end{enumerate}



\textbf{[5.13]} Let $(X(t), Y(t))$ be a two-dimensional standard Brownian motion; that is, let $\{X(t)\}$ and $\{Y(t)\}$ be standard Brownian motion processes that are independent of each other. Let $b > 0$, and define $\tau = \inf\{t : X(t) = b\}$. Find the probability density function of $Y(\tau)$. That is, find the probability density of the height at which the two-dimensional Brownian motion first hits the vertical line $x = b$.

\textit{[Hint: The answer is a Cauchy distribution.]}

\textcolor{blue}{
Let  
$
\tau=\inf\{t>0:X(t)=b$ when $b>0,
$  
so $(X(\tau),Y(\tau))=(b,Y(\tau))$ is the first point where the two-dimentional Brownian motion hits $x=b$.  
Since $X$ and $Y$ are independent one–dimensional Brownian motions started at $0$, $Y(\tau)$ can be obtained.
From the reflection principle,  
$$
f_\tau(t)\;=\;\frac{b}{\sqrt{2\pi}}\;t^{-3/2}\,e^{-b^{2}/(2t)},\quad \text{when } t>0
$$  
% \textbf{2.  Conditional law of $Y(\tau)$ given $\tau=t$.}  
For fixed $t$, $Y(t)$ is independent of $X$ and satisfies  
$$
Y(\tau)\mid\{\tau=t\}\;\sim\;\mathcal N\!\bigl(0,t\bigr)
$$
$$
g_t(y)\;=\;\frac{1}{\sqrt{2\pi t}}\;e^{-y^{2}/(2t)},\qquad y\in\mathbb R
$$  
Using the law–of–total–probability and Fubini,
$$
\begin{aligned}
f_{Y(\tau)}(y)
&=\int_{0}^{\infty}g_t(y)\,f_\tau(t)\,dt\\[6pt]
&=\int_{0}^{\infty}
      \frac{1}{\sqrt{2\pi t}}\;e^{-y^{2}/(2t)}
      \;\frac{b}{\sqrt{2\pi}}\;t^{-3/2}\,e^{-b^{2}/(2t)}\,dt\\[6pt]
&=\frac{b}{2\pi}\int_{0}^{\infty}t^{-2}\,
e^{-(b^{2}+y^{2})/(2t)}\,dt
\end{aligned}
$$
Evaluating the integral by substitution $u=(b^{2}+y^{2})/(2t)$, s.t.
$t=(b^{2}+y^{2})/(2u)$ and $dt=-(b^{2}+y^{2})/(2u^{2})\,du$,
$$
\begin{aligned}
\int_{0}^{\infty}t^{-2}e^{-(b^{2}+y^{2})/(2t)}\,dt
&=\int_{\infty}^{0}\Bigl(\frac{2u}{b^{2}+y^{2}}\Bigr)^{2}
      e^{-u}\;\Bigl(-\frac{b^{2}+y^{2}}{2u^{2}}\Bigr)\,du\\[6pt]
&=\frac{1}{\tfrac{1}{2}(b^{2}+y^{2})}\int_{0}^{\infty}e^{-u}\,du\\[6pt]
&=\frac{2}{b^{2}+y^{2}}
\end{aligned}
$$
Therefore,
$$
f_{Y(\tau)}(y)=\frac{b}{2\pi}\;\frac{2}{b^{2}+y^{2}}
              =\frac{b}{\pi\bigl(b^{2}+y^{2}\bigr)},\qquad \forall \ y\in\mathbb R
$$  
% \textbf{4.  Identification with the Cauchy distribution.}  
Finally, see that the density
$
y\;\longmapsto\;\frac{b}{\pi\bigl(b^{2}+y^{2}\bigr)}
$
is the centered Cauchy density with $b$. Thus,
$$
Y(\tau)\;\sim\;\text{Cauchy}(0,b)
$$
}


\textbf{[5.15]} Let $0 < s < t < u$.

\begin{enumerate}
    \item[(a)] Show that $\mathbb{E}(W_s W_t \mid W_u) = \frac{s}{t} \mathbb{E}(W_t^2 \mid W_u)$.

    \textcolor{blue}{
Suppose. $0<s<t<u$. Note that $\{W_r\}_{r\ge 0}$ is a centred Gaussian process with independent increments.  
Since $W_u$ is non--degenerate, any finite vector built from the path is jointly Gaussian. As such, conditional expectations can be obtained by linear regression.  
% A convenient way to organize the calculation is to decompose the \emph{Brownian bridge}
Let us decompose the Brownian bridge.
$$
W_r=\frac{r}{u}\,W_u+B_r,\qquad 0\le r\le u
$$
where $\{B_r\}_{0\le r\le u}$ is a mean–$0$ Gaussian bridge independent of $W_u$ with covariance  
$$
\mathbb{E}[B_rB_{r'}]=\frac{r(u-r')}{u},\qquad r\le r'
$$
% \medskip
Calculating the conditional momemt of $W_t$.
% \textbf{Step 1:  Conditional second moment of $W_t$.}
$$
W_t^2=\Bigl(\frac{t}{u}\,W_u\Bigr)^2+2\frac{t}{u}\,W_uB_t+B_t^2
$$
Note that $B_t$ is independent of $W_u$ and has mean $0$.
Taking conditional expectation given $W_u$ yields
$$
\mathbb{E}(W_t^2\mid W_u)=\frac{t^2}{u^2}\,W_u^2+\mathbb{E}(B_t^2)=\frac{t^2}{u^2}\,W_u^2+\frac{t(u-t)}{u}
$$
\medskip
% \textbf{Step 2:  Conditional mixed moment $\mathbb{E}(W_sW_t\mid W_u)$.}
Using the same decomposition,
$$
% \begin{aligned}
W_sW_t=\Bigl(\frac{s}{u}W_u+B_s\Bigr)\Bigl(\frac{t}{u}W_u+B_t\Bigr)=\frac{st}{u^2}W_u^2+\frac{s}{u}W_uB_t+\frac{t}{u}W_uB_s+B_sB_t
% \end{aligned}
$$
Conditioning on $W_u$ removes the linear terms in $B_s,B_t$ and replaces $B_sB_t$ with its covariance,
$$
\mathbb{E}(W_sW_t\mid W_u)=\frac{st}{u^2}W_u^2+\mathbb{E}(B_sB_t)
                           =\frac{st}{u^2}W_u^2+\frac{s(u-t)}{u}
$$
% \medskip
% \textbf{Step 3:  Relation asserted in part (a).}
Therefore,
$$
\frac{s}{t}\,\mathbb{E}(W_t^2\mid W_u)
       =\frac{s}{t}\Bigl(\frac{t^2}{u^2}W_u^2+\frac{t(u-t)}{u}\Bigr)
       =\frac{st}{u^2}W_u^2+\frac{s(u-t)}{u}
       =\mathbb{E}(W_sW_t\mid W_u)
$$
Thus,
$$
\mathbb{E}(W_sW_t\mid W_u)=\frac{s}{t}\,\mathbb{E}(W_t^2\mid W_u)
$$
    }



    \item[(b)] Find $\mathbb{E}(W_t^2 \mid W_u)$ [you know $\mathrm{Var}(W_t \mid W_u)$ and $\mathbb{E}(W_t \mid W_u)$!] and use this to show that
    \[
    \mathrm{Cov}(W_s, W_t \mid W_u) = \frac{s(u-t)}{u}.
    \]

    \textcolor{blue}{
Recall that we have already shown that 
$$
\mathbb{E}(W_t^2\mid W_u)=\frac{t(u-t)}{u}+\frac{t^2}{u^2}W_u^2
$$
For conditional covariance,
$$
\mathrm{Cov}(W_s,W_t\mid W_u)
=\mathbb{E}(W_sW_t\mid W_u)-\mathbb{E}(W_s\mid W_u)\,\mathbb{E}(W_t\mid W_u)
$$
Since $\{W_r\}$ is a martingale,
$$
\mathbb{E}(W_r\mid W_u)=\frac{r}{u}\,W_u\qquad\text{for some }0\le r\le u
$$
As such,
$$
\mathbb{E}(W_s\mid W_u)\,\mathbb{E}(W_t\mid W_u)=\frac{st}{u^2}W_u^2
$$
Thus,
$$
\mathrm{Cov}(W_s,W_t\mid W_u)=\frac{s}{t}(\frac{t(u-t)}{u}+\frac{t^2}{u^2}W_u^2)-\frac{st}{u^2}W_u^2=\frac{s(u-t)}{u}
$$
    }
\end{enumerate}






\textbf{[5.17]} Verify that the definitions (\textbf{5.13}) and (\textbf{5.14}) give Brownian bridges.

% Here is an easy way to manufacture a Brownian bridge from a
% standard Brownian motion: define

\[
\text{(5.13)} \quad X(t) = W(t) - tW(1) \quad \text{for } 0 \leq t \leq 1.
\]

% It is easy and pleasant to verify that the process $X$ defined this
% way satisfies the definition of a Brownian bridge; I wouldn’t dream
% of denying you the pleasure of checking it for yourself! Notice
% that, given the construction of standard Brownian motion $W$, now
% we do not have to worry about the existence or construction of the
% Brownian bridge. Another curious but sometimes useful fact is that
% the definition

\[
\text{(5.14)} \quad Y(t) = (1 - t)W\left( \frac{t}{1 - t} \right) \quad \text{for } 0 \leq t < 1, \quad Y(1) = 0
\]

% also gives a Brownian bridge.


\textcolor{blue}{
% \[
% \textbf{Solution.  Definition of a Brownian bridge on }[0,1].
% \]
Recall that a centred, continuous Gaussian process $\{B(t)\}_{0\le t\le 1}$ is called a Brownian bridge given that 
\[
    B(0)=B(1)=0,\quad \text{and} \quad
    \operatorname{Cov}\bigl(B(s),B(t)\bigr)=\min\{s,t\}-st,\;\text{ given }0\le s,t\le 1
\]
When $s\le t$, covariance is $s(1-t)$.
To demonstrate that (5.13) is Brownian bridge, consider the required properties. \\
\textit{(1) End points.}
$X(0)=W(0)=0,\;\;X(1)=W(1)-W(1)=0$. \\
\textit{(2) Gaussianity.}
$X(t)$ is a fixed linear combination of $\{W(r)\}_{0\le r\le 1}$, 
 so every finite‐dimensional distribution is multivariate normal. \\
\textit{(3) Mean.}
$\mathbb E[X(t)]=\mathbb E[W(t)]-t\mathbb E[W(1)]=0.$ \\
\textit{(4) Covariance. }For $0\le s\le t\le 1$,
\[
\begin{aligned}
\operatorname{Cov}(X(s),X(t))
&=\mathbb E[(W(s)-sW(1))(W(t)-tW(1))] =\min\{s,t\}-t s-s(1)+st =s(1-t)
\end{aligned}
\]
See that this is $\min\{s,t\}-st$. \\
\textit{(5)  Continuity.} 
$X$ inherits almost–sure continuity from $W$. \\
Given that all
axioms are satisfied, $X$ is a Brownian bridge. \\ \\ \\
\bigskip
To demonstrate that (5.14) is Brownian bridge, consider the required properties. \\
%
% \[
% \textbf{(5.14)}\;
% Y(t)=
% \begin{cases}
% (1-t)\,W\!\bigl(\dfrac{t}{1-t}\bigr), & 0\le t<1,\\[2mm]
% 0, & t=1.
% \end{cases}
% \]
% \[
% \textit{Claim : }Y\text{ is a Brownian bridge.}
% \]
Let $\theta(t)=\dfrac{t}{1-t}$, s.t. $\theta:[0,1)\to[0,\infty)$ is strictly increasing. \\
\textit{(1) End points.} 
$Y(0)=(1-0)W(0)=0,\qquad 
Y(1)=0$ by definition. \\
\textit{(2) Gaussianity.}
For $t<1$, $Y(t)$ is a scalar multiple of $W(\theta(t))$; any finite vector $(Y(t_1),\dots,Y(t_k))$
is thus a linear image of $(W(\theta(t_1)),\dots,W(\theta(t_k)))$, and is Gaussian. \\
\textit{(3) Mean.}
Trivially, $\mathbb E[Y(t)]=0$ \\
\textit{(4) Covariance. Fix }$0\le s\le t<1.$
$\theta(s)\le\theta(t)\;\Longrightarrow\;
\min\{\theta(s),\theta(t)\}=\theta(s)$
\[
\begin{aligned}
\operatorname{Cov}(Y(s),Y(t))
&=(1-s)(1-t)\,
      \mathbb E\!\Bigl[W\!\bigl(\theta(s)\bigr)\,W\!\bigl(\theta(t)\bigr)\Bigr]=(1-s)(1-t)\,\theta(s)=(1-s)(1-t)\,\frac{s}{1-s}=s(1-t)
\end{aligned}
\]
Therefore, $\operatorname{Cov}(Y(s),Y(t))=\min\{s,t\}-st$ $\forall s,t\le1$.
\textit{(5)  Continuity and the value at }$t=1.$
Computing the variance, $\operatorname{Var}[Y(t)]=t(1-t)\xrightarrow[t\to1^-]{}0$.  
As such, $Y(t)\to0$ is in $L^2$ and therefore in probability as $t\to1^-$.  
Since $W$ admits a continuous modification, we can choose a modification and confirm $t\mapsto Y(t)$ is continuous w.p. 1 on $[0,1)$ and converges to $0$ at $t=1$. 
Therefore, redefining $Y(1)=0$ yields a continuous version on $[0,1]$. \\
Thus, $X$ and $Y$ both satisfy the defining properties of a Brownian bridge on $[0,1]$.
% Consequently, (5.13) and (5.14) indeed “manufacture’’ Brownian bridges from a single standard Brownian motion.
}




\textbf{Problem 1.} (15 points) 
Let $W(t), t \geq 0$ be a standard
Brownian motion. Prove that it is a Gaussian process, i.e., for all
$n \in \mathbb{N}, t_1, \dots, t_n \geq 0$ and $a_1, \dots, a_n \in
\mathbb{R}$, the distribution of $\sum_{i=1}^{n} a_i W(t_i)$ is
Gaussian.

\textcolor{blue}{
Recall the definition of standard Brownian motion $\{W(t)\}_{t\ge0}$,
$$
W(0)=0,\quad
\text{for }0\le s<t\;,\;W(t)-W(s)\sim N(0,t-s),\quad \text{s.t. }\;
\{W(t)-W(s)\}_{0\le s<t}\text{ are independent}
$$
Let
$
n\in\mathbb N,
t_1,\dots,t_n\ge0,
a_1,\dots,a_n\in\mathbb R,
$  
and set  
$
S=\sum_{i=1}^{n}a_iW(t_i).
$  
% \textbf{Step 1.  Reduction to strictly increasing times.}  
Note that if some of the $t_i$ are equal, we can merge coefficients and if they are unordered,
we can re-label indices s.t.
$$
0\le t_{(1)}<t_{(2)}<\dots<t_{(m)},\qquad
b_j:=\sum_{i:\,t_i=t_{(j)}}a_i,\quad 1\le j\le m\le n,
$$
and denote $S=\sum_{j=1}^{m}b_jW(t_{(j)})$.  
Hence WLOG $0<t_1<\dots<t_n$.
% \textbf{Step 2.  Express $S$ in terms of increments.}  
Defining the independent Gaussian increments as
$$
\Delta_1:=W(t_1)-W(0)=W(t_1)\quad \text{and} \quad
\Delta_k:=W(t_k)-W(t_{k-1}),\;2\le k\le n
$$
As such, $S$ can be rewritten as
$$
\begin{aligned}
S
&=\sum_{i=1}^{n}a_i\Bigl(\Delta_1+\cdots+\Delta_i\Bigr)
 =\sum_{k=1}^{n}\Bigl(\sum_{i=k}^{n}a_i\Bigr)\Delta_k
 =\sum_{k=1}^{n}c_k\Delta_k
\end{aligned}
$$
where $c_k=\sum_{i=k}^{n}a_i$. \\
% \textbf{Step 3.  Use closure of the Gaussian family under linear combinations.}  
Note that each increment $\Delta_k$ is Gaussian.
$$
\Delta_k\sim N\bigl(0,t_k-t_{k-1}\bigr)\quad\text{s.t. }t_0=0
$$
and the vector $(\Delta_1,\dots,\Delta_n)$ has independent components through the definition of Brownian motion.  
Because independent Gaussian variables are also jointly Gaussian, any deterministic linear combination of them must also be Gaussian.
$
S=\sum_{k=1}^{n}c_k\Delta_k
$
is a sum of independent $N(0,\sigma_k^2)$ variables multiplied by deterministic scalars $c_k$. As such,
$$
S\sim N\!\Bigl(0,\;\sum_{k=1}^{n}c_k^{\,2}(t_k-t_{k-1})\Bigr)
$$
or $S$ is Gaussian.
% \textbf{Step 4.  Characteristic–function verification (optional but instructive).}
% For completeness, compute the characteristic function of $S$:
% $$
% \begin{aligned}
% \varphi_S(\lambda)
% &=\mathbb E\,e^{i\lambda S}
%  =\prod_{k=1}^{n}\mathbb E\exp\!\bigl(i\lambda c_k\Delta_k\bigr)\\
% &=\prod_{k=1}^{n}\exp\!\Bigl(-\tfrac12\lambda^{2}c_k^{2}(t_k-t_{k-1})\Bigr)
%  =\exp\!\Bigl(-\tfrac12\lambda^{2}\sum_{k=1}^{n}c_k^{\,2}(t_k-t_{k-1})\Bigr),
% \end{aligned}
% $$
% the characteristic function of a centred normal distribution, confirming the previous step.
% \bigskip
% \textbf{Conclusion.}  For arbitrary $n$, times $t_1,\dots,t_n\ge0$ and coefficients $a_1,\dots,a_n\in\mathbb R$, the linear form $S=\sum_{i=1}^{n}a_iW(t_i)$ is Gaussian.  Therefore the finite–dimensional distributions of $\{W(t)\}$ are multivariate normal, so $W$ is indeed a \emph{Gaussian process}.
}

\end{document}
