\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{parskip}


\usepackage[a4paper, margin=1in]{geometry}

\title{S\&DS 351: Stochastic Processes - Homework 3}
\author{Bryan SebaRaj \\[0.8em] Professor Ilias Zadik}
\date{February 7, 2025}

\begin{document}

\maketitle

\subsection*{Problem 1} (10 points) Is it possible for a transient state to be periodic? If so, construct an example of such a Markov chain; otherwise, give a mathematical proof why not.

\textcolor{blue}{Note: I (fortunately) solved this after proving problem 3, so for a more thorough proof on how this example is transient, please see Problem 3. \\ \\ 
Yes, it is possible for a transient state to be periodic. Consider a 1-dimensional asymmetric random walk on $\mathbb{Z}$:
\[
   X_{n} \;=\; X_{n-1} \;+\; Z_n,
   \quad\text{where}\quad
   \mathbb{P}(Z_n = +1) \;=\; p
   \quad\text{and}\quad
   \mathbb{P}(Z_n = -1) \;=\; 1 - p
\]
for some $p \in (0,1)$ with $p \neq \tfrac{1}{2}$. Starting at state 0, state 0 is transient (see Problem 3). \\ \\ 
% We claim that in this Markov chain:
% \begin{enumerate}
%   \item \emph{State 0 is transient} whenever $p \neq \tfrac{1}{2}.$
%   \item \emph{State 0 has period 2}, regardless of $p$ (as long as $p>0$ and $1-p>0$).
% \end{enumerate}
\smallskip
\noindent
Define the period as $d_i = \text{gcd}\{n:P^n(i,i)>0\}$, where $P$ is the transition matrix. \\
In the random walk, the walk must trivially take as many $+1$ steps as $-1$ steps to reach the initial state. Thus one can only return to state $x$ starting from $x$ in an even number of steps. Note that this holds for all integers. Hence for each integer $x$, 
\[
  (P^n)(x,x) \;>\;0 
  \quad\Longrightarrow\quad 
  n \ \text{is even}
\]
\[
  (P^n)(x,x) \;=\;0 
  \quad\Longrightarrow\quad 
  n \ \text{is odd}
\]
Therefore, the greatest common divisor of all such $n$ is $2$, and every state $x\in\mathbb{Z}$ has period~$2$.
}
   
\subsection*{Problem 2} Let $X_0, X_1, \dots$ be a Markov chain with transition matrix $P$. Let $k \geq 1$ be an integer.
\begin{enumerate}
    \item[(a)] (5 points) Prove that $Y_n = X_{kn}$ is also a Markov chain. Find its transition matrix.

            \textcolor{blue}{
                Since $Y_n = X_{kn}$, the conditional probability for $Y_{n+1}$ can be defined as
\[
\mathbb{P}\left(X_{k(n+1)} = y_{n+1} \mid X_{kn} = y_n, X_{k(n-1)} = y_{n-1}, \dots, X_0 = y_0\right)
\]
Because $\{X_n\}$ is a Markov chain, it satisfies the Markov property,
 \[
P\bigl(X_{m+1} = x_{m+1} \,\big\vert\, X_m = x_m, \dots, X_0 = x_0\bigr)
= P\bigl(X_{m+1} = x_{m+1} \,\big\vert\, X_m = x_m\bigr)
\]
Applying for the $k$-steps from time $kn$ to time $k(n+1)$,
\[
\mathbb{P}\left(X_{k(n+1)} = y_{n+1} \mid X_{kn} = y_n, X_{k(n-1)} = y_{n-1}, \dots, X_0 = y_0\right)
= \mathbb{P}\left(X_{k(n+1)} = y_{n+1} \mid X_{kn} = y_n\right)
\]
Therefore,
\[
\mathbb{P}\left(Y_{n+1} = y_{n+1} \mid Y_n = y_n, Y_{n-1} = y_{n-1}, \dots, Y_0 = y_0\right)
= \mathbb{P}\left(Y_{n+1} = y_{n+1} \mid Y_n = y_n\right)
\]
Thus, $\{Y_n\}$ satisfies the Markov property and is a Markov chain. \\ \\ 
Solving for the transition matrix, note that for any state $i,j \in S$,
\[
\mathbb{P}(Y_{n+1} = j \mid Y_n = i) = \mathbb{P}(X_{k(n+1)} = j \mid X_{kn} = i)
\]
In the Markov chain $\{X_n\}$, 
\[
\mathbb{P}(X_{k(n+1)} = j \mid X_{kn} = i) = (P^k)_{ij}
\]
Thus, the one-step transition probability for $\{Y_n\}$ is
\[
\mathbb{P}(Y_{n+1} = j \mid Y_n = i) = (P^k)_{ij}
\]
Therefore, the transition matrix for $\{Y_n\}$ is $P^k$.
% By the definition $Y_n = X_{kn}$,
% \[
% P\bigl(Y_{n+1} = j \,\big\vert\, Y_0 = i_0, \dots, Y_n = i_n\bigr)
% = P\bigl(X_{k(n+1)} = j \,\big\vert\, X_{0} = i_0, X_k = i_1, \dots, X_{kn} = i_n\bigr).
% \]
% Since $\{X_n\}$ is a Markov chain, it satisfies the Markov property:
% \[
% P\bigl(X_{m+1} = x_{m+1} \,\big\vert\, X_0 = x_0, \dots, X_m = x_m\bigr)
% = P\bigl(X_{m+1} = x_{m+1} \,\big\vert\, X_m = x_m\bigr).
% \]
% Applying this property step-by-step for the transitions from time $kn$ to time $k(n+1)$ (i.e., $k$ steps), we obtain:
% \[
% P\bigl(X_{k(n+1)} = j \,\big\vert\, X_{kn} = i_n\bigr) = \bigl(P^k\bigr)(i_n, j).
% \]
% Because all intermediate states between $X_{kn}$ and $X_{k(n+1)}$ do not affect this probability beyond the conditioning on $X_{kn}$, we conclude:
% \[
% P\bigl(X_{k(n+1)} = j \,\big\vert\, X_{0} = i_0, \dots, X_{kn} = i_n\bigr)
% = P\bigl(X_{k(n+1)} = j \,\big\vert\, X_{kn} = i_n\bigr)
% = \bigl(P^k\bigr)(i_n, j).
% \]
% Hence,
% \[
% P\bigl(Y_{n+1} = j \,\big\vert\, Y_0 = i_0, \dots, Y_n = i_n\bigr)
% = P\bigl(Y_{n+1} = j \,\big\vert\, Y_n = i_n\bigr),
% \]
% proving that $\{Y_n\}$ is a Markov chain. \\ \\
% Finally, by the computation above, the one-step transition probability for $Y_n$ from state $i_n$ to state $j$ is exactly $\bigl(P^k\bigr)(i_n, j)$. Therefore, the transition matrix of $\{Y_n\}$ is $P^k$.
}

        
        \item[(b)] (10 points) Suppose that the original chain $\{X_n\}$ is irreducible. Is $\{Y_n\}$ irreducible? If so, prove it; if not, provide a counterexample.

            \textcolor{blue}{
            Consider a Markov chain, with state space $S=\{X_0, X_1\}$. Define its transition matrix as $$P_X=
  \left( {\begin{array}{cc}
   0 & 1 \\
   1 & 0 \\
  \end{array} } \right)$$
  Since $X_0$ and $X_1$ communicate, this chain is trivially irreducible. \\ \\ 
  Suppose $k=2$. The transition matrix of $Y_m=X_{2n}$ is $$P_Y=P_X^2=\left( {\begin{array}{cc}
   1 & 0 \\
   0 & 1 \\
  \end{array} } \right)$$
  $X_0$ and $X_1$ do not communicate in the new chain, as $X_0$ and $X_1$ are closed subsets on $S$. Therefore, $Y_n$ is not irreducible. \\ \\ 
  Note that this can be generalized to any Markov chain $X_0, X_1,...,X_k$, where $\mathbb{P}_X(i,j)=1$ for $j = (i + 1) \text{ mod } k$ and $\mathbb{P}_X(i,j)=0$ for all other $i$, for all $j$. This chain is trivially irreducible, and yields a transition matrix $P^k$ which is a $k\times k$ identity matrix. The chain $Y_n$ is not irreducible, as the states $X_0, X_1,...,X_k$ are closed subsets on $S$.
               }

        
        \item[(c)] (10 points) Suppose that the original chain $\{X_n\}$ is aperiodic. Is $\{Y_n\}$ aperiodic? If so, prove it; if not, provide a counterexample.

            \textcolor{blue}{
Recall that the period of a state~$i$ is defined as
\[
   d(i) \;=\; \gcd\!\Bigl\{\, n \ge 1 : (P^n)_{ii} > 0 \Bigr\}
\]
If this $\gcd$ equals~$1$ for every $i$, then the chain is aperiodic.  \\ 
In the chain $\{Y_n\}$ defined by $Y_n = X_{kn}$, the transition probabilites are given by
\[
  \mathbb{P}(Y_{m+1} = j \mid Y_m = i)
  \;=\; \mathbb{P}(X_{k(m+1)} = j \mid X_{km} = i)
  \;=\; \bigl(P^k\bigr)_{ij}
\]
Hence the transition matrix of $\{Y_n\}$ is exactly $P^k$, and the $m$-step transition probabilities in $\{Y_n\}$ are given by $(P^k)^m = P^{km}$. \\ \\
Therefore, the period of state~$i$ as a state of the chain $\{Y_n\}$ is
\[
   d_Y(i) \;=\; \gcd\!\Bigl\{\, m \ge 1 : \bigl(P^{km}\bigr)_{ii} > 0 \Bigr\}
\]
A standard characterization of aperiodicity is that for each state $i$, there exists some integer $N$ st 
\[
  (P^n)_{ii} \;>\; 0 \quad \forall n \ge N
\]
In other words, $\{\,n : (P^n)_{ii} > 0\}$ is co‐finite, or contains all sufficiently large~$n$. \\ \\
If $(P^n)_{ii} > 0$ for all $n\ge N$, then in particular $(P^{km})_{ii} > 0$ whenever $km \ge N$.  Hence for all integers
   $m \;\ge\; \bigl\lceil N/k \bigr\rceil$,
$(P^{km})_{ii} > 0.$  \\ \\ 
Thus,
   $\Bigl\{\, m : \bigl(P^{km}\bigr)_{ii} > 0 \Bigr\}$
contains the tail set $\{\lceil N/k\rceil, \lceil N/k\rceil +1, \lceil N/k\rceil+2, \dots\}$ of all sufficiently large integers $m$. \\ \\
\noindent
For the chain $\{Y_n\}$, the period of $i$ is 
\[
  d_Y(i) 
  \;=\; \gcd\!\Bigl\{\, m \ge 1 : \bigl(P^{km}\bigr)_{ii} > 0 \Bigr\}
\]
As this set includes all sufficiently large integers $m$, the gcd of any infinite set must be~$1$. \\ \\ 
Since $d_Y(i)=1$ for every state $i$, $\{Y_n\}$ is aperiodic.
}
        
        \item[(d)] (10 points) Suppose that the original chain $\{X_n\}$ is transient. Is $\{Y_n\}$ transient? If so, prove it; if not, provide a counterexample.

%             \textcolor{red}{
%                 Recall that a Markov chain $\{X_n\}$ is said to be \emph{transient} if for every state $i$ in its state space,
% \[
% \mathbb{P}\bigl(\text{the chain visits state $i$ infinitely many times}\bigr) = 0.
% \]
% Equivalently, each state $i$ is visited only finitely many times with probability $1$. \\ 
% Now, let $\{Y_n\}$ be the $k$-step chain defined by $Y_n := X_{kn}$. We show that if $\{X_n\}$ is transient, then so is $\{Y_n\}$. \\ 
% Fix any state $i$ in the state space. Define the event $A_i := \{\text{$Y_n = i$ for infinitely many $n$}\}$. Observe that if $Y_n = i$ for infinitely many $n$, it means $X_{kn} = i$ for infinitely many $n$. Hence, the event $A_i$ can occur only if the original chain $\{X_n\}$ visits the state $i$ \emph{infinitely often} at the times $\{k,2k,3k,\dots\}$. \\ 
% However, by transience of $\{X_n\}$, we know that the probability that $X_n$ visits $i$ infinitely often (at \emph{any} set of time indices) is $0$. In particular,
% \[
% \mathbb{P}\Bigl(\{\text{$X_n = i$ infinitely often}\}\Bigr) = 0.
% \]
% Since $\{\text{$X_{kn} = i$ infinitely often}\} \subseteq \{\text{$X_n = i$ infinitely often}\}$, it follows that
% \[
% \mathbb{P}(A_i)
% \;=\;
% \mathbb{P}\bigl(\{\text{$Y_n = i$ infinitely often}\}\bigr)
% \;\le\;
% \mathbb{P}\bigl(\{\text{$X_n = i$ infinitely often}\}\bigr)
% \;=\; 0.
% \]
% Therefore, for each state $i$, the probability that $i$ is visited infinitely many times by the chain $\{Y_n\}$ is $0$. This shows that $\{Y_n\}$ is also transient.
%                 observe that $\{P^{kn}(i,i)\}_{n=0}^\infty$ is a subsequence of the original sequence $\{P^m(i,i)\}_{m=0}^\infty$ with indices restricted to $m = kn$. Specifically:
% \[
% \sum_{n=0}^\infty P^{kn}(i,i) = \sum_{m \in \{0, k, 2k, \dots\}} P^m(i,i).
% \]
% Since $P^m(i,i) \geq 0$ for all $m \geq 0$, the series $\sum_{m=0}^\infty P^m(i,i)$ is a sum of non-negative terms. By the **comparison test for series**, if $\sum_{m=0}^\infty a_m$ converges and $0 \leq b_m \leq a_m$ for all $m$, then $\sum_{m=0}^\infty b_m$ also converges. Here, take:
% \[
% a_m = P^m(i,i), \quad b_m = \begin{cases} 
% P^m(i,i) & \text{if } m \text{ is a multiple of } k, \\
% 0 & \text{otherwise}.
% \end{cases}
% \]
% Then $\sum_{m=0}^\infty b_m = \sum_{n=0}^\infty P^{kn}(i,i)$. Since $\sum_{m=0}^\infty a_m < \infty$ (by transience of $\{X_n\}$) and $0 \leq b_m \leq a_m$ for all $m$, it follows that:
% \[
% \sum_{n=0}^\infty P^{kn}(i,i) \leq \sum_{m=0}^\infty P^m(i,i) < \infty.
% \]
% Thus, the expected number of visits to state $i$ for $\{Y_n\}$ is finite. Since this holds for all states $i \in S$, the chain $\{Y_n\}$ is transient.
% }
            \textcolor{blue}{
Recall that a Markov chain $\{X_n\}$ on $S$ is transient if there exists at least one state $i \in S$ st 
\[
\mathbb{P}_i\bigl(\text{the chain ever returns to } i\bigr) < 1
\]
Equivalently,
\[
\mathbb{P}_i\bigl(\exists n \ge 1: X_n = i\bigr) < 1
\]
Consider state $i \in S$ and suppose we start from $X_0 = i$. \\  
By transience of $\{X_n\}$, 
\[
\mathbb{P}_i\bigl(\exists m \ge 1 : X_m = i\bigr) < 1
\]
Define the event 
\[
A = \{\exists m \ge 1 : X_m = i\}
\]
and the event 
\[
B = \{\exists n \ge 1 : Y_n = i\} = \{\exists n \ge 1 : X_{kn} = i\}
\]
Note that if $X_{kn} = i$ for some $n$, $X_m = i$ for $m = kn$, i.e. $B \subseteq A$. Hence,
\[
\mathbb{P}_i(B) \;\le\; \mathbb{P}_i(A)
\]
Since $A$ has probability strictly less than 1,
\[
\mathbb{P}_i\bigl(\exists n \ge 1 : Y_n = i\bigr) \;=\; \mathbb{P}_i(B) \;<\; 1
\]
Therefore, $i$ is transient for the chain $\{Y_n\}$ as well. Since this applies to every state $i \in S$ that is transient in $\{X_n\}$, it shows that no state $i$ can become recurrent under the $k$‐step sampling. The probability that $\{Y_n\}$ returns to $i$ is bounded above by the probability that $\{X_n\}$ returns to $i$, and the latter is less than 1 for transient states. 
}

        
        \item[(e)] (15 points) Suppose that the original chain $\{X_n\}$ is recurrent. Is $\{Y_n\}$ recurrent? If so, prove it; if not, provide a counterexample.

%             \textcolor{blue}{By definition of recurrence, for any state $i \in S$:
% \[
% \sum_{m=0}^\infty P^m(i,i) = \infty,
% \]
% where $P^m(i,i)$ is the probability of returning to $i$ in $m$ steps.
% \vspace{0.5em}
% \noindent \textbf{Step 2: Transition Structure of $\{Y_n\}$} \\
% The process $\{Y_n\}$ has transition matrix $P^k$ because:
% \[
% Y_{n+1} = X_{k(n+1)} = X_{kn + k}.
% \]
% Thus, the $n$-step transition probability for $\{Y_n\}$ is:
% \[
% P_Y^n(i,i) = (P^k)^n(i,i) = P^{kn}(i,i).
% \]
% \vspace{0.5em}
% \noindent \textbf{Step 3: Expected Visits for $\{Y_n\}$} \\
% The expected number of visits to $i$ for $\{Y_n\}$ is:
% \[
% \sum_{n=0}^\infty P_Y^n(i,i) = \sum_{n=0}^\infty P^{kn}(i,i).
% \]
% This is a subseries of $\sum_{m=0}^\infty P^m(i,i)$ where only terms with $m = kn$ are included.
% \vspace{0.5em}
% \noindent \textbf{Step 4: Divergence of the Subseries} \\
% Assume for contradiction that $\sum_{n=0}^\infty P^{kn}(i,i) < \infty$. Since $P^m(i,i) \geq 0$ for all $m$, we have:
% \[
% \sum_{m=0}^\infty P^m(i,i) \geq \sum_{n=0}^\infty P^{kn}(i,i).
% \]
% But this implies $\sum_{m=0}^\infty P^m(i,i) < \infty$, which contradicts the recurrence of $\{X_n\}$. Therefore:
% \[
% \sum_{n=0}^\infty P^{kn}(i,i) = \infty.
% \]
% \vspace{0.5em}
% \noindent \textbf{Step 5: Conclusion for $\{Y_n\}$} \\
% Since $\sum_{n=0}^\infty P_Y^n(i,i) = \infty$ for all $i \in S$, every state $i$ is recurrent for $\{Y_n\}$. Thus, $\{Y_n\}$ is recurrent.}
%
\textcolor{blue}{Let $\{X_n\}_{n\ge 0}$ be a Markov chain on a state space $\mathcal{S}$ with transition matrix $P$, and assume that $\{X_n\}$ is recurrent. Fix state $i\in\mathcal{S}$ that is recurrent, st
\[
\mathbb{P}_i\Bigl(\{n\ge 1 : X_n = i\} =\infty \Bigr)=1
\]
For $k\ge 1$, define the chain $\{Y_n\}_{n\ge 0}$ by
\[
Y_n=X_{kn},\quad n=0,1,2,\dots
\]
Since $i$ is recurrent for $\{X_n\}$, define the set $A$ as
\[
A=\{n\ge 1 : X_n=i\}=\infty
\]
Now, note that every positive integer belongs to one of the $k$ residue classes modulo $k$, or
\[
\mathbb{N}=\bigcup_{r=0}^{k-1}\{n\in\mathbb{N} : n\equiv r\pmod{k}\}
\]
Thus, the return times can be partitioned as
\[
A=\bigcup_{r=0}^{k-1}A_r,\quad \text{where } A_r=\{n\in A : n\equiv r\pmod{k}\}
\]
Because $A$ is countably infinite, by the pigeonhole principle at least one of the sets $A_r$ must be coutably infinite. As such, there exists an $r_0\in\{0,1,\dots,k-1\}$ st
\[
\mathbb{P}_i\Bigl( A_{r_0} =\infty \Bigr)=1
\]
Consider the two cases, \\ \\ 
\medskip
Case 1: If $r_0=0$, then infinitely many returns to $i$ occur at times that are multiples of $k$. In other words,
\[
A_0=\{n\ge 1 : X_n=i \text{ and } n\equiv 0\pmod{k}\}=\infty
\]
% \[
% \{n\ge 1 : Y_n=i\}=\{n\ge 1 : X_{kn}=i\}
% \]
% which is exactly $A_0$. 
Hence, $i$ is visited infinitely often by the chain $\{Y_n\}$ and is therefore recurrent. \\ \\ 
% \medskip
Case 2: If $r_0\neq 0$, define a time-shifted chain
\[
\widetilde{Y}_n=X_{kn+r_0} % ,\quad n=0,1,2,\dots.
\]
Since $\widetilde{Y}_n$ is merely a shifted version of the $k$-chain, it is also a Markov chain, with the same state space and transition matrix $P^k$, and has the same recurrence properties, as recurrence is invariant under a finite time shift. Since $A_{r_0}$ is infinite, 
\[
\{n\ge 0 : \widetilde{Y}_n=i\}=\{n\ge 0 : X_{kn+r_0}=i\}=\infty
\]
Therefore, $i$ is recurrent for the chain $\{\widetilde{Y}_n\}$. Since $\widetilde{Y}_n$ and $Y_n$ differ only by a fixed time shift, it follows that recurrence holds for both chains. \\ \\ 
Hence, $i$ is recurrent for $\{Y_n\}$ as well.
In either case, if $i$ is recurrent for $\{X_n\}$, then it is also recurrent for $\{Y_n\}$. \\ \\ 
Since recurrence is a class property, the entire chain $\{Y_n\}$ must be recurrent.
}

        
        \item[(f)] (5 points) Suppose that the original chain $X_n$ is irreducible and that it has period $d$. What is the period of each state $i$ in the new Markov chain $Y_n$ for $k = d$?

           \textcolor{blue}{Since the original chain is irreducible with period~$d$, for each state~$i$,
\[
    (P^d_X)_{ii} \;>\; 0 \quad \text{and} \quad (P^{d-b}_X)_{ii} \;=\; 0, \quad \forall b = 1, 2, \dots, d-1
\]
Therefore, for any multiple of $d$, $a\in \mathbb{Z}$ such that $a \geq 1$, $$(P^d_X)^a_{ii} > 0$$
As such, all returns to $i$ can occur only at multiples of~$d$ steps.   \\ \\ 
Define the transition matrix for $Y_n$ where $k=d$ as
\[
  \mathbb{P}(Y_{m+1} = j \,\big|\; Y_m = i)
    \;=\; (P^k_X)_{ij}
  \;=\; (P^d_X)_{ij}
\]
When $j=i$, 
\[
  (P^1_Y)_{ii} 
    \;=\; (P^d_X)_{ii}
  \;>\; 0
\]
As such, state $i$ can reach itself in a single step in chain $Y$, forming a self-loop in Y. \\ \\ 
Defining the period of $i$ in $Y$,
\[
  d_Y(i)
  \;=\;
  \gcd\,\bigl\{\, m \;\ge\; 1 : (P^{d\,m})_{ii} > 0 \bigr\}
\]
Note that $(P^d)_{ii} > 0$, so at $m=1$, $(P^{d\cdot 1})_{ii} = (P^d)_{ii} > 0$. Since the chain is irreducible, raising $P^d$ to a higher power trivially cannot change its positivity:
\[
   (P^{d\,2})_{ii} = \bigl(P^d\bigr)^2_{ii} > 0, 
   \quad
   (P^{d\,3})_{ii} > 0, 
   \;\;\dots
\]
$(P^{dn})_{ii} > 0$ holds for all $n \ge 1$, so the set of possible $m$ is $\{1,2,3,\dots\}$.
Thus,
\[
  \bigl\{\,m : (P^{d\,m})_{ii} > 0 \bigr\}
  \;=\;
  \{1, 2, 3, \dots\}
\]
with a trivial gcd of 1. Note that since the $d$ period was a state property in the original chain, $\{X_n\}$, it holds for all states in the old chain, and thus all states in the new chain. \\ \\ 
As such, the period of each state in the new chain $\{Y_n\}$ is $1$. In other words, the chain $\{Y_n\}$ is aperiodic.
}
    \end{enumerate}

\subsection*{Problem 3} (Asymmetric random walk, 15 points) Consider the \textit{asymmetric} random walk on $\mathbb{Z}$, that is, $X_n = X_{n-1} + Z_n$, where $Z_1, Z_2, \dots$ are iid and $\mathbb{P}(Z_n = +1) = p$ and $\mathbb{P}(Z_n = -1) = 1 - p$, with $p \in [0,1]$ and $p \neq \frac{1}{2}$. Show that the state 0 is a transient state.

    In Lecture 7 we saw/will see that when $p = \frac{1}{2}$ this is not true anymore and the state 0 is recurrent. Can you explain intuitively why this is the case?

    \textit{Hint:} You may want to use Stirling’s formula that $\lim_{n \to \infty} \frac{n!}{(n/e)^n \sqrt{2\pi n}} = 1$.


    \textcolor{blue}{
Starting from \(X_0 = 0\), the random walk is at state \(0\) again at \(t=n\) only when it has taken an equal number of \(+1\) steps as \(-1\) steps. As such, \(n\) must be even. \\ \\ Suppose \(n = 2k\), and \(k\) is the number of \(Z_i\) that are \(+1\),
\[
  \mathbb{P}(X_{2k} = 0 \,\big\vert\, X_0 = 0)
  \;=\;
  \binom{2k}{k}\, p^k\, (1-p)^k
\]
Note that $\mathbb{P}(X_n = 0 \,\big\vert\, X_0 = 0) \;=\; 0$ if $n$ is odd. \\ \\
So the series of return probabilities at \(0\) is
\[
  \sum_{n=0}^\infty \mathbb{P}(X_n = 0 \,\big\vert\, X_0 = 0)
  \;=\;
  1 \;+\; \sum_{k=1}^\infty \binom{2k}{k}\, p^k\, (1-p)^k
\]
accounting for the initial state of 0. \\ Using Stirling's approximation,
\medskip
\noindent
\[
  n! \;\sim\; \Bigl(\tfrac{n}{e}\Bigr)^{n}\sqrt{2\pi n}
  \quad\text{as }n \to \infty
\]
applying to this case,
\[
  \binom{2k}{k}
  \;=\;
  \frac{(2k)!}{k!\,k!}
  \;\approx\;
  \frac{\sqrt{4\pi k}\,\bigl(\tfrac{2k}{e}\bigr)^{2k}}{\,2\pi k\,\bigl(\tfrac{k}{e}\bigr)^{k}\bigl(\tfrac{k}{e}\bigr)^{k}}
  \;=\;
  \frac{4^k}{\sqrt{\pi k}}
\]
Therefore,
\[
  \binom{2k}{k}\,p^k\,(1-p)^k
  \;\approx\;
  \frac{4^k}{\sqrt{\pi k}} \,\bigl[p(1-p)\bigr]^k
  \;=\;
  \frac{\bigl[4\,p(1-p)\bigr]^k}{\sqrt{\pi k}}
\]
\medskip
\noindent
If $p \neq \frac{1}{2}$, then \(4\,p(1-p) < 1\) (If $f(x)=x(1-x)$, then $f'(x)=-x+1-x=-2x+1$. Solving for the max when $f'(x)=0, x=\frac{1}{2}$). \\ \\ 
Note, that as \(k \to \infty\), \(\bigl[4\,p(1-p)\bigr]^k\) decays exponentially. Therefore,
\[
  \binom{2k}{k}\,p^k\,(1-p)^k
  \;=\;
  O\!\Bigl(\bigl[4\,p(1-p)\bigr]^k\Bigr)
  \quad
  \text{and}
  \quad
  \sum_{k=1}^{\infty} \binom{2k}{k}\,p^k\,(1-p)^k
  \;<\;\infty
\]
Thus,
\[
  \sum_{n=0}^\infty \mathbb{P}(X_n = 0 \,\big\vert\, X_0 = 0)
  \;=\;
  1 \;+\;
  \sum_{k=1}^{\infty}
  \binom{2k}{k}\, p^k\,(1-p)^k
  \;<\;
  \infty
\]
which defines a transient state. \\ \\ 
\medskip
\noindent
However, when \(p = \frac{1}{2}\),
\[
  \binom{2k}{k}\,\Bigl(\frac12\Bigr)^k\,\Bigl(\frac12\Bigr)^k
  \;\approx\;
    \frac{\bigl[4\cdot 0.5(1-0.5)\bigr]^k}{\sqrt{\pi k}}
  \;=\;
  \frac{1}{\sqrt{\pi k}}
\]
so
\[
  \sum_{k=1}^\infty \frac{1}{\sqrt{k}}
  \;=\;
  \infty
\]
which defines a recurrent state when $p=\frac12$. }

    \subsection*{Exercise 1.8} Consider a Markov chain on the integers with
\[
P(i, i+1) = 0.4 \text{ and } P(i, i-1) = 0.6 \text{ for } i > 0,
\]
\[
P(i, i+1) = 0.6 \text{ and } P(i, i-1) = 0.4 \text{ for } i < 0,
\]
\[
P(0,1) = P(0,-1) = \frac{1}{2}.
\]

This is a chain with infinitely many states, but it has a sort of probabilistic ``restoring force'' that always pushes back toward 0. Find the stationary distribution.

\textcolor{blue}{
Denote the stationary distribution by $\{\pi_i\}_{i\in\mathbb{Z}}$
\[
\sum_{j \in \mathbb{Z}} \pi_j P(j,i) \;=\; \pi_i
\quad \forall i \in \mathbb{Z}
\]
and $\sum_{i \in \mathbb{Z}} \pi_i = 1$.
As this is a two-sided birth-death chain,
\[
\pi_i \, P(i, i+1) \;=\; \pi_{i+1} \, P(i+1, i)
\]
For $i \ge 1$,
\[
\pi_i \times 0.4 \;=\; \pi_{i+1} \times 0.6
\quad \Longrightarrow \quad 
\frac{\pi_{i+1}}{\pi_i} = \frac{0.4}{0.6} = \frac{2}{3}
\]
For $i \le -1$,
\[
    \pi_{i-1} \times 0.6 \;=\; \pi_{i} \times 0.4
\quad \Longrightarrow \quad 
\frac{\pi_{i-1}}{\pi_i} = \frac{0.4}{0.6} = \frac{2}{3}
\]
When $i=0$,
\[
\pi_0 \times 0.5 \;=\; \pi_1 \times 0.6
\quad \Longrightarrow \quad
\frac{\pi_1}{\pi_0} = \frac{0.5}{0.6} = \frac{5}{6}
\]
\[
\pi_{-1} \times 0.6 = \pi_0 \times 0.5
\quad \Longrightarrow \quad 
\frac{\pi_{-1}}{\pi_{0}} = \frac{0.5}{0.6} = \frac{5}{6}
\]
Hence,
\[
\pi_1 = \frac{5}{6}\,\pi_0, 
\quad
\pi_{-1} = \frac{5}{6}\,\pi_0
\]
As the probabilities of a jump remain the same, generalizing for all $i\geq1$,
\[
\pi_{i+1} = \frac{2}{3} \pi_{i}
\quad\Longrightarrow\quad 
\pi_i = \left(\frac{2}{3}\right)^{\,i-1} \,\pi_1
\quad \text{for } i \ge 1
\]
\[
\pi_i 
\;=\; 
\left(\frac{2}{3}\right)^{\,i-1} \cdot \frac{5}{6}\,\pi_0
\quad \forall i \ge 1
\]
Generalizing for all $i\leq-1$,
\[
    \pi_{i-1} = \frac{2}{3} \pi_{i}
\quad\Longrightarrow\quad
\pi_{i} = \frac{2}{3}^{-i-1}\,\pi_{-1}
\quad \forall i\leq -1
\]
\[
    \pi_{i} 
\;=\; 
\left(\frac{2}{3}\right)^{\,-i-1} \cdot \frac{5}{6}\,\pi_0
\quad \forall i \leq -1
\]
Combining the two cases,
\[
\pi_i 
\;=\;
\frac{5}{6}\,\left(\frac{2}{3}\right)^{\,|i|-1}\,\pi_0, \quad \forall i \neq 0
\]
Solving for $\pi_0$, first recall that,
\[
\sum_{i=-\infty}^{\infty} \pi_i \;=\; 1
\]
Hence,
\[
\pi_0 
\;+\;
\sum_{i\neq 0} \frac{5}{6} \,\biggl(\tfrac{2}{3}\biggr)^{|i|-1}\,\pi_0
\;=\; 1
\]
\[
\pi_0 
\biggl[
1 
\;+\; 
\frac{5}{6}\sum_{i\neq 0}\bigl(\tfrac{2}{3}\bigr)^{|i|-1}
\biggr]
\;=\; 1
\]
Exploiting the symmetry of the chain,
\[
\sum_{i\neq 0}\bigl(\tfrac{2}{3}\bigr)^{|i|-1} 
\;=\;
2 \sum_{j=1}^{\infty} \bigl(\tfrac{2}{3}\bigr)^{j-1}
\;=\;
2 \sum_{k=0}^{\infty} \bigl(\tfrac{2}{3}\bigr)^{k}
\;=\;
2 \cdot \frac{1}{1 - \tfrac{2}{3}}
\;=\;
2 \cdot 3
\;=\;
6
\]
Thus
\[
\pi_0 \Bigl[\,1 + \frac56 \cdot 6\,\Bigr] 
\;=\;
6\,\pi_0
\;=\;
1 
\quad\Longrightarrow\quad
\pi_0 
\;=\; 
\frac{1}{6}
\]
Therefore,
\[
\pi_i
=
\begin{cases}
\displaystyle \frac{1}{6},
& i=0\\[6pt]
\displaystyle \frac{5}{36}\,\bigl(\tfrac{2}{3}\bigr)^{|i|-1}
& i \neq 0
\end{cases}
\]
}


\subsection*{Exercise 1.16} Show that if an irreducible Markov chain has a state $i$ such that $P(i, i) > 0$, then the chain
is aperiodic. Also show by example that this sufficient condition is not necessary.

\textcolor{blue}{
Let $\{X_n\}$ be an irreducible Markov chain on a countable state space $S$. Suppose there is a state $i \in S$ such that
\[
    P(i,i) > 0
\]
The period of state $i$ is defined as
\[
    d(i) \;=\; \gcd \{ n \ge 1 : P^n(i,i) > 0 \}
\]
Since $P(i,i) = P^1(i,i)> 0$, there is a positive probability of returning to $i$ in exactly 1 step. As such, $1 \in \{n: P^n(i,i) > 0\}$, so any common divisor of all $n$ must trivially divide 1. \\ \\ Therefore,
\[
    d(i) \;=\; \gcd \{ n \ge 1 : P^n(i,i) > 0 \} \;=\; 1
\]
Thus, $i$ is an aperiodic state. \\ \\ 
By irreducibility, for any other state $j \in S$, $\exists m,k \in \mathbb{Z}$ such that $ m,k\ge 1$, $P^m(j,i) > 0$, and $P^k(i,j) > 0$. Therefore as proven in lecture, for any $n \ge 1$,
\[
    P^{m+n+k}(j,j) 
    \;\ge\; P^m(j,i) \, P^n(i,i) \, P^k(i,j)
\]
Since $P^n(i,i) > 0$ for all $n \ge 1$, for arbitrarily many $n$, the probability $P^{m+n+k}(j,j)$ is strictly positive. As such,
\[
    d(j) \;=\; \gcd\{ n \ge 1 : P^n(j,j) > 0 \} \;=\; 1
\]
Thus, every state in an irreducible chain with a self-loop, i.e. $P(i,i) > 0$, is aperiodic. \\ \\ 
To show that this sufficient condition is not necessary, consider a chain on three states $\{1,2,3\}$ with transition matrix:
\[
    P(1,1)=0, \quad P(1,2)=1, \quad P(1,3)=0 
\]
\[
    P(2,1)=\tfrac{1}{2}, \quad P(2,2)=0, \quad P(2,3)=\tfrac{1}{2}
\]
\[
    P(3,1)=\tfrac{1}{2}, \quad P(3,2)=\tfrac{1}{2}, \quad P(3,3)=0
\]
Note that the chain is irreducible, but does not contain any self-loops, i.e. $P(i,i)=0$ for $i=1,2,3$. \\ \\ 
\[
    P^2(1,1) = P(1,2)P(2,1) = (1)\left(\tfrac{1}{2}\right) = \tfrac{1}{2} > 0
\]
\[
    P^3(1,1) = P(1,2)P(2,3)P(3,1) = 1 \cdot \tfrac{1}{2} \cdot \tfrac{1}{2}
    = \tfrac{1}{4} > 0
\]
Therefore, state $1$ can return to itself in both $2$ steps and $3$ steps, which comprises a subset of the set of all possible return times back to state 1. The gcd of $2$ and $3$ is $1$, or a period of $1$. 
Note that $P^2(2,2)>0$, $P^2(3,3)>0$, and $P^3(2,2)>0$, $P^3(3,3)>0$, so every state also has period $1$, making the entire chain aperiodic.
Thus, $P(i,i) > 0$ for some $i$ is not a necessary condition for a chain to be aperiodic.
}


\end{document}

