\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{xcolor}

\usepackage[a4paper, margin=1in]{geometry}


\title{S\&DS 351: Stochastic Processes - Homework 1}
\author{Bryan SebaRaj \\[0.8em] Professor Ilias Zadik}
\date{January 22, 2025}

\begin{document}

\maketitle

\section*{Problem 1.1}
Suppose you have a matrix $X \in \mathbb{R}^{m \times n}$ and another matrix $Y \in \mathbb{R}^{n \times p}$. Let $Z = X \times Y$, i.e., the matrix multiplication of $X$ and $Y$.

\begin{enumerate}[label=(\alph*)]
    \item (5 points) What are the dimensions of $Z$? What is the $i,j$th entry of $Z$ in terms of those of the matrices $X$ and $Y$? Is $Z$ necessarily equal to $Y \times X$? If not, provide a counterexample.

        \textcolor{blue}{The dimensions of $Z \in \mathbb{R}^{m \times p}$. \\ The $i,j$th entry of $Z$ is given by 
        $$Z_{i,j}=\sum_{k=1}^{n}A_{i,k}B_{k,j}$$ \\ 
         $Z$ is not necessarily equal to $Y \times X$. For example, consider the following matrices:
         $$X = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \quad Y = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}$$
         Then, $Z = X \times Y = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}$, but $Y \times X = \begin{bmatrix} 23 & 34 \\ 31 & 46 \end{bmatrix}$.
     \\[0.6em] Therefore, there $\exists X, Y$ such that $X \times Y = Z \neq Y \times X$.}
    
    \item (5 points) Consider the following matrix $P$:
    \[
    P =
    \renewcommand{\arraystretch}{1.2}
    \begin{bmatrix}
        0 & 0 & 1 \\
        \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
        0 & \frac{1}{2} & \frac{1}{2}
    \end{bmatrix}.
    \]
    Find $P^2$ (that is, $P \times P$).
    \renewcommand{\arraystretch}{1.2}
    \textcolor{blue}{$$P^2=P\times P = \begin{bmatrix}
        0 & 0 & 1 \\
        \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
        0 & \frac{1}{2} & \frac{1}{2}
    \end{bmatrix} \begin{bmatrix}
        0 & 0 & 1 \\
        \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
        0 & \frac{1}{2} & \frac{1}{2}
    \end{bmatrix}= \begin{bmatrix}
        0 & \frac{1}{2} & \frac{1}{2} \\
        \frac{1}{9} & \frac{5}{18} & \frac{11}{18} \\
        \frac{1}{6} & \frac{5}{12} & \frac{5}{12}
    \end{bmatrix}$$}

    \item (5 points) Find the limit of $P^n$ as $n \to \infty$ (that is, find the limit of each entry $(P^n)_{i,j}$, $1 \leq i,j \leq 3$ as $n \to \infty$). You do not need to prove what the limit is; it suffices to guess correctly (using a calculator or computer is allowed).

        \textcolor{blue}{Since $P$ is row-stochastic, $$ \lim_{x\to\infty} P^n=\begin{bmatrix} \pi \\ \pi \\ \pi\end{bmatrix}=\mathbf{1}\pi ^ {\top}$$
            where $\pi$ is the stationary distribution statisfying $\pi P = \pi$ and $\pi_1 + \pi_2 + \pi_3 = 1$. \\ 
        Let $\pi = (\pi_1, \pi_2, \pi_3)$. $$(\pi P)_j=\sum_{i=1}^{3}\pi_i P_{i,j}=\pi_j$$ for each $j=1,2,3$ \\ 
        For $j=1$, $\pi_1 = 0\pi_1 + \frac{1}{3}\pi_2 + 0\pi_3 = \frac{1}{3}\pi_2$ \\
        So, $\pi_1 = \frac{1}{3}\pi_2$ \\
        For $j=2$, $\pi_2 = 0\pi_1 + \frac{1}{3}\pi_2 + \frac{1}{2}\pi_3 = \frac{1}{3}\pi_2 + \frac{1}{2}\pi_3$, or $\pi_3=\frac{4}{3}\pi_2$ \\
        For $j=3$, $\pi_3 = \pi_1 + \frac{1}{3}\pi_2 + \frac{1}{2}\pi_3 = \pi_1 + \frac{1}{3}\pi_2 + \frac{1}{2}\pi_3$ \\
        Normalizing using $\pi_1 + \pi_2 + \pi_3 = 1$, $$\pi_1 + \pi_2 + \pi_3 = \frac{1}{3}\pi_2 + \pi_2 + \frac{4}{3}\pi_2=\frac{8}{3}\pi_2=1$$ \\
    Therefore, $\pi_2 = \frac{3}{8}$, $\pi_1 = \frac{1}{3}\pi_2 = \frac{1}{8}$, and $\pi_3 = \frac{4}{3}\pi_2 = \frac{1}{2}$. \\
$$\pi=\left( \frac{1}{8}, \frac{3}{8}, \frac{1}{2} \right)$$ \\ 
     In a $3 \times 3$ matrix, where every row is $\pi$, we get 
     \[
\lim_{n \to \infty} P^n = 
\begin{bmatrix} 
    \frac{1}{8} & \frac{3}{8} & \frac{1}{2} \\ 
    \frac{1}{8} & \frac{3}{8} & \frac{1}{2} \\ 
    \frac{1}{8} & \frac{3}{8} & \frac{1}{2} 
\end{bmatrix}
\]}


\item (Bonus, 10 points) Prove the following statement for any $P \in \mathbb{R}^{3 \times 3}$. Assume the limit of $P^n$ as $n \to \infty$ equals a matrix of the form $\mathbf{1}\pi^\top$ for some $\pi \in \mathbb{R}^{3 \times 1}$ and $\mathbf{1} = (1, 1, 1)^\top \in \mathbb{R}^{3 \times 1}$. Confirm that $\mathbf{1}^\top \pi \in \mathbb{R}^{3 \times 3}$. Prove that $P^\top \pi = \pi$.

        % \textcolor{blue}{Since $P$ is row-stochastic and the property is preserved under matrix multiplication, $$P^n\mathbf{1}=\mathbf{1}, \ \forall n$$
    %     Therefore, $$\lim_{n \to \infty} P^n\mathbf{1}=\mathbf{1}$$
    % Given that $\lim_{n \to \infty} P^n = \mathbf{1}\pi^\top$, $$\mathbf{1}\pi^\top\mathbf{1}=\mathbf{1}(\pi^\top\mathbf{1})=\mathbf{1}$$
% Therefore, $\pi^\top\mathbf{1}=1$ and $\mathbf{1}^\top\pi=1$}


\end{enumerate}


\section*{Problem 1.2}
Suppose that we are given two geometric random variables $A_1$ and $A_2$ with parameter $p$ which are not necessarily independent. Let $\{B_1, B_2, \dots\}$ be a sequence of random variables independent of $A_1$ and $A_2$, such that each $B_i$ has mean $\mu$ and variance $\sigma^2$.

\begin{enumerate}[label=(\alph*)]
    \item (5 points) Compute $\mathbb{E}[A_1 + 300A_2]$.

    \textcolor{blue}{Given that $A_1$ and $A_2$ are geometric, even if they are not independet, the expectation of a sum of random variables is the sum of their expectations. As such
    $$\mathbb{E}[A_1 + 300A_2]=\mathbb{E}[A_1] + 300\mathbb{E}[A_2]=\frac{1}{p}+\frac{300}{p}=\frac{301}{p}$$}

    \item (5 points) Prove that $\mathbb{P}[A_1 + 300A_2 \geq 5000/p] \leq 0.1$.

        \textcolor{blue}{Employing Markov's inequality, where for any non-negative random variable $X$ and any $a>0$, $$\mathbb{P}[X \geq a] \leq \frac{\mathbb{E}[X]}{a}$$
        $$\mathbb{P}\left( A_1 + 300A_2 \geq \frac{5000}{p} \right) \leq \frac{\mathbb{E}[A_1 + 300A_2]}{\frac{5000}{p}}=\frac{\frac{301}{p}}{\frac{5000}{p}}=\frac{301}{5000}$$
    Since $\frac{301}{5000} < 0.1$, $\mathbb{P}[A_1 + 300A_2 \geq 5000/p] \leq \frac{301}{5000} < 0.1$, proving the inequality.}


    \item (10 points) Compute $\mathbb{E}[\sum_{i=1}^{A_1} B_i^2]$. (Hint: condition on $A_1$).

        \textcolor{blue}{From the given information, $$\mathbb{E}[B^2_i]=Var(B_i)+(\mathbb{E}[B_i])^2 = \sigma^2 + \mu^2$$
        Using the law of total expectation and conditioning on $A_1$, $$\mathbb{E}[\sum_{i=1}^{A_1} B_i^2]=\mathbb{E}[\mathbb{E}[\sum_{i=1}^{A_1} B_i^2|A_1]]=\mathbb{E}[A_1(\sigma^2 + \mu^2)]=(\sigma^2 + \mu^2)\mathbb{E}[A_1]$$
        Since $A_1 \sim $ Geometric$(p)$, $$\mathbb{E}[A_1]=\frac{1}{p}$$  
        Therefore, $$\mathbb{E}\left[ \sum_{i=1}^{A_1}B^2_i \right] = \frac{\sigma^2 + \mu^2}{p}$$}

\end{enumerate}

\section*{Problem 1.3}
Suppose that two teams play a best of 5 series. That is, whichever team wins 3 games is the winner of the series. Suppose that each game is played independently, and for each game team $A$ has a probability 0.7 of winning and team $B$ has a probability 0.3.

\begin{enumerate}[label=(\alph*)]
    \item (5 points) What is the probability that team $A$ wins the series?
    \item (5 points) What is the probability that team $A$ wins the series conditioned on the fact that team $B$ won the first game? If you had to bet, would you bet on $A$ winning the series? Would you still bet on $A$ winning after $B$ won the first game?
\end{enumerate}

\section*{Problem 1.4}
Let $X, Y$ be two \textit{independent} standard normal random variables. Let $R$ be an exponential random variable with parameter 1 and let $\Theta$ be a uniform random variable taking values between $[0, 2\pi]$.

\begin{enumerate}[label=(\alph*)]
    \item (5 points) Compute $\mathbb{P}(R = 0)$ (please note that this isnâ€™t the PDF of $R$ at 0, we are asking what is the probability that $R$ equals 0).
    \item (10 points) Compute $\mathbb{P}(X^2 + Y^2 \geq t)$.
    \item (10 points) Assume that $R$ and $\Theta$ are independent. Define $A = \sqrt{R}\cos(\Theta)$ and $B = \sqrt{R}\sin(\Theta)$, what is the joint PDF of $A, B$? What is the marginal PDF of $A$?
\end{enumerate}

\section*{Problem 1.5}
Suppose that $X \sim \text{Exp}(\lambda_1)$, $Y \sim \text{Exp}(\lambda_2)$, and $Y$ is independent of $X$.

\begin{enumerate}[label=(\alph*)]
    \item (5 points) Compute $\mathbb{P}(X > Y)$.
    \item (5 points) Compute $\mathbb{P}(X > (t + x) \mid X > t)$, for $t > 0$ and $x > 0$.
    \item (5 points) Compute $\mathbb{P}(\min(X, Y) > t)$.
\end{enumerate}

\section*{Problem 1.6}
Given a fair die with 8 possible sides, let $T$ be the number of times you have to roll so that all eight sides have appeared at least once. Let $N$ be the number of distinct sides obtained from the first eight rolls.

\begin{enumerate}[label=(\alph*)]
    \item (5 points) Find $\mathbb{E}(T)$.
    \item (5 points) Find $\mathbb{E}(N)$.
    \item (5 points) Find $\mathbb{E}(T \mid N = 4)$.
\end{enumerate}


\end{document}

