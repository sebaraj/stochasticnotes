\documentclass{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{parskip}


\usepackage[a4paper, margin=1in]{geometry}

\title{S\&DS 351: Stochastic Processes - Homework 4}
\author{Bryan SebaRaj \\[0.8em] Professor Ilias Zadik}
\date{February 14, 2025}

\begin{document}

\maketitle

1. (Nearest-neighbor walk in two dimensions)
Let $\{X_n : n \geq 0\}$ denote the nearest-neighbor walk on $\mathbb{Z}^2$: given $X_n = (a,b)$, the next position $X_{n+1}$ is equally likely to be one of \textbf{four} possibilities: $(a+1,b)$, $(a-1,b)$, $(a,b+1)$, or $(a,b-1)$, as shown in the following picture.

\begin{center}
\includegraphics[scale=0.2]{/Users/bryansebaraj/Desktop/Screenshot 2025-02-09 at 2.26.54 PM.png}
\end{center}

\begin{enumerate}[label=(\alph*)]
    \item (8 points) Let $u_n$ denote the probability of returning to the origin in $n$ steps if the chain starts from the origin. For integer $m \geq 0$, show that $u_{2m+1} = 0$ and

\[
u_{2m} = \frac{1}{4^{2m}} \sum_{k=0}^{m} \frac{(2m)!}{\{k!(m-k)!\}^2}
\]

(Hint: the walk either moves horizontally or vertically. If $2k$ steps are horizontal, then $2(m-k)$ steps are vertical.)

\item (5 points) Use the fact that 
$\sum_{k=0}^{m} \binom{m}{k}^2 = \binom{2m}{m}$
to arrive at the simple expression

\[
u_{2m} = \frac{1}{4^{2m}} \binom{2m}{m}^2.
\]

\item (10 points) As we did in class, use Stirling’s approximation $n! \sim \left(\frac{n}{e}\right)^n \sqrt{2\pi n}$ as $n \to \infty$ to conclude that $\sum_{n \geq 0} u_n = \infty$. (Hint: recall the fact mentioned in class: $\sum_{m \geq 1} m^{-\alpha} = \infty$ if $\alpha \leq 1$ and $< \infty$ if $\alpha > 1$.)

\item (2 points) Use the classification criterion to conclude that the chain is \textit{recurrent}.
\end{enumerate}

2. (Nearest-neighbor walk in three dimensions)
Now that we have warmed up, let’s consider $\{X_n : n \geq 0\}$ being the nearest-neighbor walk on $\mathbb{Z}^3$: given $X_n = (a,b,c)$, the next position $X_{n+1}$ is equally likely to be one of \textbf{six} possibilities: $(a+1,b,c)$, $(a-1,b,c)$, $(a,b+1,c)$, $(a,b-1,c)$, $(a,b,c+1)$, or $(a,b,c-1)$.

\begin{enumerate}[label=(\alph*)]
    \item (8 points) Again, let $u_n$ denote the probability of returning to the origin in $n$ steps if the chain starts from the origin. For integer $m \geq 0$, show that $u_{2m+1} = 0$ and

\[
u_{2m} = \frac{1}{6^{2m}} \sum_{k=0}^{m} \sum_{j=0}^{m-k} \frac{(2m)!}{\{j! k! (m-j-k)!\}^2}
\]

(Hint: Mimic the reasoning from part (a) in Problem 1, this time in three dimensions.)

\item (5 points) Simplify the above expression as

$$u_{2m} = \frac{1}{2^{2m}} \binom{2m}{m} \sum_{k=0}^{m} \sum_{j=0}^{m-k} p_{k,j}, 
\text{ where }
p_{k,j} = \frac{1}{3^m} \binom{m}{k} \binom{m-k}{j}.
$$

\item (10 points) Show that

\[
\sum_{k=0}^{m} \sum_{j=0}^{m-k} p_{k,j} = 1.
\]

(Hint: how many ways to distribute $m$ balls into 3 urns?)

\item (Bonus, 5 points) Using Stirling’s approximation, one can show that

\[
\max_{k,j} p_{k,j} \leq \frac{C}{m},
\]

for some positive constant $C$, where the maximum is over all $k,j \geq 0$ such that $k+j \leq m$.

\item (5 points) Combine parts (b), (c), (d) and use Stirling’s approximation to show that 
$u_{2m} \leq \frac{C'}{m^{3/2}}$
for some constant $C'$ and $\sum_{n \geq 0} u_n < \infty$.

\item (2 points) Use the classification criterion to conclude that the chain is \textit{transient}.
\end{enumerate}

3. Let $i$ be a recurrent state of a Markov Chain. Assume another state $j$ is accessible from $i$.

\begin{enumerate}[label=(\alph*)]
    \item (3 points) Prove that 

\[
\mathbb{P}(N_i = \infty | X_0 = i) = 1.
\]

\item (10 points) Prove that 

\[
\mathbb{P}(T_j < \infty | X_0 = i) = 1.
\]

\item (5 points) Prove that 

\[
\mathbb{P}(T_i < \infty | X_0 = j) = 1.
\]

\item (2 points) Is it true that $j$ needs also to be recurrent?
\end{enumerate}

4. Let $j$ be a transient state of a Markov chain and $i$ an arbitrary state of the Markov chain.
 
\begin{enumerate}[label=(\alph*)]
    \item (10 points) Prove that for some constant $C > 0$ and some $p \in (0,1)$ it holds for all $t \geq 1$
\[
\mathbb{P}(N_j \geq t | X_0 = i) = 1 \leq C p^t.
\]

\item (5 points) Conclude that 
\[
\lim_{n \to \infty} P^n_{i,j} = 0.
\]
\end{enumerate}

Chang 1.25. Prove the following proposition:  \\
The total variation distance $\|\lambda - \mu\|$ may also be expressed in the alternative forms:

\[
\|\lambda - \mu\| = \sup_{A \subseteq S} |\lambda(A) - \mu(A)| = \frac{1}{2} \sum_{i \in S} |\lambda(i) - \mu(i)| = 1 - \sum_{i \in S} \min\{\lambda(i), \mu(i)\}.
\]

\end{document}


\end{document}

